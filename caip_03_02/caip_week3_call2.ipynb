{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CAIP Week 3 Call 2: AWS Bedrock API Integration and Error Handling\n",
        "\n",
        "## Overview\n",
        "This notebook covers hands-on implementation of AWS Bedrock API calls, handling throttling errors, implementing retry logic, and processing multiple requests efficiently. This builds on the conceptual foundation from Week 3 Call 1.\n",
        "\n",
        "### Key Learning Objectives:\n",
        "- Understand how to call Bedrock models programmatically\n",
        "- Learn to handle ThrottlingException errors\n",
        "- Implement exponential backoff retry logic\n",
        "- Apply rate limiting between API calls\n",
        "- Use prompt templates for batch processing\n",
        "- Parse and extract responses from Bedrock API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setting Up the Bedrock Runtime Client\n",
        "\n",
        "### Key Point: Use the Runtime Client\n",
        "\n",
        "**Critical distinction**: To invoke models for inference, you must use `boto3.client('bedrock-runtime')`, NOT `boto3.client('bedrock')`.\n",
        "\n",
        "### Why Two Different Clients?\n",
        "- **`bedrock` client**: For managing model resources, listing available models, configuring access\n",
        "- **`bedrock-runtime` client**: For actually invoking models and getting predictions\n",
        "\n",
        "### Example Setup:\n",
        "```python\n",
        "import boto3\n",
        "import json\n",
        "\n",
        "# Create the Bedrock runtime client\n",
        "bedrock = boto3.client('bedrock-runtime')\n",
        "```\n",
        "\n",
        "**Note**: The runtime client is what you use to send prompts and receive responses from models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Constructing the Request Body\n",
        "\n",
        "### The `construct_body()` Function\n",
        "\n",
        "The request body must be properly formatted for Claude models on Bedrock. Here's what it requires:\n",
        "\n",
        "### Required Fields:\n",
        "1. **`anthropic_version`**: Must be `\"bedrock-2023-05-31\"` for Claude models\n",
        "2. **`max_tokens`**: Maximum number of tokens in the response (default: 2000)\n",
        "3. **`messages`**: Array containing the user message with role and content\n",
        "\n",
        "### Example Implementation:\n",
        "```python\n",
        "def construct_body(prompt: str, max_tokens: int = 2000) -> Dict[str, Any]:\n",
        "    body = {\n",
        "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Human: {prompt}\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "    return body\n",
        "```\n",
        "\n",
        "### Key Points:\n",
        "- **anthropic_version**: This is a fixed string required by AWS Bedrock for Anthropic models\n",
        "- **max_tokens**: Controls response length - remember you pay for every token!\n",
        "- **messages**: The content follows Anthropic's format with `\"Human: {prompt}\"`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Calling Bedrock Models\n",
        "\n",
        "### The `call_bedrock()` Function\n",
        "\n",
        "This function handles the complete flow of calling a Bedrock model:\n",
        "\n",
        "### Process Flow:\n",
        "1. Construct the request body\n",
        "2. Invoke the model with `invoke_model()`\n",
        "3. Parse the JSON response\n",
        "4. Extract text content from the response\n",
        "\n",
        "### Example Implementation:\n",
        "```python\n",
        "def call_bedrock(\n",
        "    bedrock_client: boto3.client, \n",
        "    prompt: str, \n",
        "    max_tokens: int = 2000, \n",
        "    modelId: str = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
        ") -> List[str]:\n",
        "    # Create the request body\n",
        "    body = construct_body(prompt, max_tokens=max_tokens)\n",
        "\n",
        "    # Invoke the model with the request body\n",
        "    response = bedrock_client.invoke_model(\n",
        "        body=json.dumps(body),  # Must be JSON string!\n",
        "        modelId=modelId,\n",
        "    )\n",
        "\n",
        "    # Parse the JSON response body\n",
        "    result = json.loads(response[\"body\"].read())\n",
        "\n",
        "    # Extract and return text content\n",
        "    responses = [content[\"text\"] for content in result[\"content\"]]\n",
        "    return responses\n",
        "```\n",
        "\n",
        "### Critical Details:\n",
        "- **`body` must be JSON string**: Use `json.dumps(body)`, not a Python dict\n",
        "- **Response parsing**: `response[\"body\"].read()` returns bytes that must be parsed\n",
        "- **Content extraction**: `result[\"content\"]` is an array, each item has a `\"text\"` field\n",
        "- **Default model**: Claude 3 Sonnet provides good balance of performance and cost\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Understanding ThrottlingException\n",
        "\n",
        "### What Happens When You Make Too Many Requests?\n",
        "\n",
        "**ThrottlingException** occurs when you exceed AWS Bedrock's rate limits by making too many API calls in a short time period.\n",
        "\n",
        "### Real-World Example from the Session:\n",
        "- **What happened**: Processing 50 US states sequentially without delays\n",
        "- **Result**: Successfully processed 47 states, then hit ThrottlingException on West Virginia (state #48)\n",
        "- **Error message**: `\"Too many requests, please wait before trying again\"`\n",
        "\n",
        "### Why Throttling Exists:\n",
        "- **Protects system resources**: Prevents overload on AWS infrastructure\n",
        "- **Ensures fair usage**: Prevents one user from monopolizing the service\n",
        "- **Maintains service quality**: Ensures consistent performance for all users\n",
        "\n",
        "### Key Insight:\n",
        "> \"Even though each individual request was legitimate, the volume of requests in a short time period exceeded the service's comfort threshold.\"\n",
        "\n",
        "### Common Mistakes:\n",
        "- Making requests in a loop without delays\n",
        "- Not implementing retry logic\n",
        "- Ignoring rate limit warnings\n",
        "- Processing large batches without rate limiting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Implementing Exponential Backoff\n",
        "\n",
        "### What is Exponential Backoff?\n",
        "\n",
        "Exponential backoff is a retry strategy that waits progressively longer between retry attempts. The wait time increases exponentially with each attempt.\n",
        "\n",
        "### Formula:\n",
        "```python\n",
        "wait_time = (2 ** attempt) + random.uniform(0, 1)\n",
        "```\n",
        "\n",
        "### Wait Times:\n",
        "- **Attempt 1**: ~2-3 seconds\n",
        "- **Attempt 2**: ~4-5 seconds  \n",
        "- **Attempt 3**: ~8-9 seconds\n",
        "\n",
        "### Why Add Random Jitter?\n",
        "\n",
        "Random jitter prevents the **\"thundering herd\" problem** where multiple clients retry at exactly the same time, causing another stampede of requests.\n",
        "\n",
        "### Example Implementation:\n",
        "```python\n",
        "import time\n",
        "import random\n",
        "from botocore.exceptions import ClientError\n",
        "\n",
        "def call_bedrock_with_retry(bedrock_client, prompt, max_retries=3):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            return call_bedrock(bedrock_client, prompt)\n",
        "        except ClientError as e:\n",
        "            error_code = e.response['Error']['Code']\n",
        "            if error_code == 'ThrottlingException':\n",
        "                if attempt < max_retries - 1:  # Don't wait on last attempt\n",
        "                    wait_time = (2 ** attempt) + random.uniform(0, 1)\n",
        "                    print(f\"Throttled! Waiting {wait_time:.1f} seconds...\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    print(f\"Max retries ({max_retries}) exceeded\")\n",
        "                    raise\n",
        "            else:\n",
        "                # Re-raise non-throttling errors immediately\n",
        "                raise\n",
        "    return None\n",
        "```\n",
        "\n",
        "### Key Best Practices:\n",
        "1. **Only retry ThrottlingException**: Other errors (ValidationException, AccessDeniedException) won't succeed on retry\n",
        "2. **Set reasonable max_retries**: 3-5 attempts is usually sufficient\n",
        "3. **Add jitter**: Prevents synchronized retries from multiple clients\n",
        "4. **Don't wait on last attempt**: No point waiting if you're about to give up\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Rate Limiting Between Requests\n",
        "\n",
        "### Why Add Delays?\n",
        "\n",
        "Even with retry logic, it's better to prevent throttling in the first place by adding small delays between API calls.\n",
        "\n",
        "### Simple Rate Limiting:\n",
        "```python\n",
        "import time\n",
        "\n",
        "for i, state in enumerate(states):\n",
        "    print(f\"Processing {state}... ({i+1}/50)\")\n",
        "    \n",
        "    # Call Bedrock\n",
        "    responses = call_bedrock_with_retry(bedrock, current_prompt)\n",
        "    \n",
        "    # Add delay between requests (except after last one)\n",
        "    if i < len(states) - 1:\n",
        "        time.sleep(1)  # 1 second delay\n",
        "```\n",
        "\n",
        "### Why 1 Second?\n",
        "- **Simple and effective**: Prevents rapid-fire requests\n",
        "- **Not too slow**: 50 states × 1 second = ~50 seconds total delay\n",
        "- **Flexible**: Can adjust based on your rate limits\n",
        "\n",
        "### Advanced Rate Limiting:\n",
        "\n",
        "For larger batches, consider longer pauses periodically:\n",
        "```python\n",
        "for i, state in enumerate(states):\n",
        "    # Longer pause every 10 states\n",
        "    if i > 0 and i % 10 == 0:\n",
        "        time.sleep(5)  # 5 second pause\n",
        "    \n",
        "    responses = call_bedrock_with_retry(bedrock, current_prompt)\n",
        "    time.sleep(1)  # Regular 1 second delay\n",
        "```\n",
        "\n",
        "### Key Takeaway:\n",
        "> \"Adding a small delay between API calls helps implement rate limiting, preventing too many requests from being sent in a short period.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Using Prompt Templates\n",
        "\n",
        "### Why Use Templates?\n",
        "\n",
        "When processing multiple similar requests, prompt templates allow you to:\n",
        "- **Reuse the same prompt structure**\n",
        "- **Substitute variables** for dynamic content\n",
        "- **Maintain consistency** across all requests\n",
        "- **Easier maintenance** - change template once, affects all uses\n",
        "\n",
        "### Example Template:\n",
        "\n",
        "**prompt_template.txt**:\n",
        "```\n",
        "Act as a flower expert. I need you to give me information on the state flower of {state_name}. The response should be in json and in this format:\n",
        "\n",
        "{\n",
        "\"state\": \"state_name\",\n",
        "\"flower\": \"flower_name\",\n",
        "\"color\": \"flower_color\"\n",
        "}\n",
        "\n",
        "Do not give any additional information even if I am wrong.\n",
        "```\n",
        "\n",
        "### Why Include Constraints and Format Requirements?\n",
        "\n",
        "The template includes constraints like:\n",
        "- **\"The response should be in json\"** - Specifies output format\n",
        "- **\"in this format\"** - Shows exact JSON schema with field names\n",
        "- **\"Do not give any additional information\"** - Prevents extra text that would break parsing\n",
        "\n",
        "**Why this matters:**\n",
        "- **Structured output**: Makes responses predictable and easy for programs to parse\n",
        "- **Automated processing**: JSON responses can be directly parsed with `json.loads()`\n",
        "- **Consistency**: All responses follow the same structure, making batch processing reliable\n",
        "- **Error prevention**: Clear format requirements reduce parsing errors\n",
        "\n",
        "Without constraints, the model might add explanatory text, use different field names, or return non-JSON responses, making automated processing difficult or impossible.\n",
        "\n",
        "### Loading and Using Templates:\n",
        "```python\n",
        "# Read template from file\n",
        "with open('prompt_template.txt', 'r') as file:\n",
        "    prompt = file.read()\n",
        "\n",
        "# Replace placeholder with actual value\n",
        "current_prompt = prompt.replace(\"{state_name}\", state)\n",
        "\n",
        "# Use in API call\n",
        "responses = call_bedrock_with_retry(bedrock, current_prompt)\n",
        "```\n",
        "\n",
        "### Why Store Templates in Separate Files?\n",
        "\n",
        "Storing templates in `.txt` files (instead of hardcoding in Python) provides:\n",
        "- **Easier updates**: Non-developers can modify prompts without touching code\n",
        "- **Cleaner code**: Keeps Python scripts focused on logic, not prompt text\n",
        "- **Version control**: Track prompt changes separately from code changes\n",
        "- **Collaboration**: Different team members can work on prompts vs. code\n",
        "\n",
        "### Benefits:\n",
        "- **Consistency**: Same prompt structure for all states\n",
        "- **Maintainability**: Update template once, affects all requests\n",
        "- **Clarity**: Template file is easier to read and modify than code\n",
        "- **Reusability**: Same template can be used for different datasets\n",
        "- **Structured output**: Constraints ensure machine-readable responses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Parsing Bedrock Responses\n",
        "\n",
        "### Response Structure\n",
        "\n",
        "Bedrock API responses follow a specific structure that must be parsed correctly:\n",
        "\n",
        "### Response Format:\n",
        "```python\n",
        "{\n",
        "    \"content\": [\n",
        "        {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": \"Actual response text here\"\n",
        "        }\n",
        "    ],\n",
        "    \"stop_reason\": \"end_turn\",\n",
        "    \"usage\": {...}\n",
        "}\n",
        "```\n",
        "\n",
        "### Extraction Process:\n",
        "```python\n",
        "# Parse the response\n",
        "result = json.loads(response[\"body\"].read())\n",
        "\n",
        "# Extract text from each content block\n",
        "responses = [content[\"text\"] for content in result[\"content\"]]\n",
        "```\n",
        "\n",
        "### Key Points:\n",
        "- **`response[\"body\"]`**: Returns a streaming body that must be read\n",
        "- **`.read()`**: Reads the bytes from the streaming body\n",
        "- **`json.loads()`**: Parses the JSON string into a Python dict\n",
        "- **`result[\"content\"]`**: Array of content blocks (usually one for text responses)\n",
        "- **`content[\"text\"]`**: The actual text response within each content block\n",
        "\n",
        "### Handling JSON Responses:\n",
        "\n",
        "If your prompt requests JSON output, you'll need to parse it:\n",
        "```python\n",
        "responses = call_bedrock_with_retry(bedrock, current_prompt)\n",
        "\n",
        "for response in responses:\n",
        "    # Parse JSON string into Python dict\n",
        "    flower_data.append(json.loads(response))\n",
        "```\n",
        "\n",
        "### Error Handling:\n",
        "Always wrap JSON parsing in try-except to handle malformed responses:\n",
        "```python\n",
        "try:\n",
        "    flower_data.append(json.loads(response))\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"Failed to parse JSON: {e}\")\n",
        "    continue\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Complete Workflow Example\n",
        "\n",
        "### Processing Multiple Items with Error Handling\n",
        "\n",
        "Here's a complete example combining all the concepts:\n",
        "\n",
        "```python\n",
        "from call_bedrock import call_bedrock\n",
        "import boto3\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "from botocore.exceptions import ClientError\n",
        "\n",
        "# Create Bedrock client\n",
        "bedrock = boto3.client('bedrock-runtime')\n",
        "\n",
        "# Load prompt template\n",
        "with open('prompt_template.txt', 'r') as file:\n",
        "    prompt_template = file.read()\n",
        "\n",
        "# List of items to process\n",
        "states = [\"Alabama\", \"Alaska\", \"Arizona\", ...]  # All 50 states\n",
        "\n",
        "flower_data = []\n",
        "\n",
        "# Retry function with exponential backoff\n",
        "def call_bedrock_with_retry(bedrock_client, prompt, max_retries=3):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            return call_bedrock(bedrock_client, prompt)\n",
        "        except ClientError as e:\n",
        "            if e.response['Error']['Code'] == 'ThrottlingException':\n",
        "                if attempt < max_retries - 1:\n",
        "                    wait_time = (2 ** attempt) + random.uniform(0, 1)\n",
        "                    print(f\"Throttled! Waiting {wait_time:.1f} seconds...\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    raise\n",
        "            else:\n",
        "                raise\n",
        "    return None\n",
        "\n",
        "# Process each state\n",
        "for i, state in enumerate(states):\n",
        "    print(f\"Processing {state}... ({i+1}/50)\")\n",
        "    \n",
        "    # Replace placeholder in template\n",
        "    current_prompt = prompt_template.replace(\"{state_name}\", state)\n",
        "    \n",
        "    try:\n",
        "        # Call with retry logic\n",
        "        responses = call_bedrock_with_retry(bedrock, current_prompt)\n",
        "        \n",
        "        # Parse and store responses\n",
        "        for response in responses:\n",
        "            flower_data.append(json.loads(response))\n",
        "        \n",
        "        print(f\"✓ Successfully processed {state}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed to process {state}: {str(e)}\")\n",
        "        continue\n",
        "    \n",
        "    # Rate limiting: delay between requests\n",
        "    if i < len(states) - 1:\n",
        "        time.sleep(1)\n",
        "\n",
        "# Save results\n",
        "with open('flower_data.json', 'w') as f:\n",
        "    json.dump(flower_data, f, indent=4)\n",
        "\n",
        "print(f\"Done! Processed {len(flower_data)} items.\")\n",
        "```\n",
        "\n",
        "### Key Components:\n",
        "1. **Client setup**: Create bedrock-runtime client\n",
        "2. **Template loading**: Read prompt template from file\n",
        "3. **Retry logic**: Exponential backoff for throttling\n",
        "4. **Rate limiting**: Delay between requests\n",
        "5. **Error handling**: Try-except to continue on failures\n",
        "6. **Response parsing**: Extract and store JSON responses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Georgia...\n",
            "{'state': 'Georgia', 'flower': 'Cherokee Rose', 'color': 'White, Gold'}\n",
            "✓ Successfully processed Georgia\n"
          ]
        }
      ],
      "source": [
        "from call_bedrock import call_bedrock\n",
        "import boto3\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "from botocore.exceptions import ClientError\n",
        "\n",
        "# Create Bedrock client\n",
        "bedrock = boto3.client('bedrock-runtime')\n",
        "\n",
        "# Load prompt template\n",
        "with open('prompt_template.txt', 'r') as file:\n",
        "    prompt_template = file.read()\n",
        "\n",
        "# Retry function with exponential backoff\n",
        "def call_bedrock_with_retry(bedrock_client, prompt, max_retries=3):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            return call_bedrock(bedrock_client, prompt)\n",
        "        except ClientError as e:\n",
        "            if e.response['Error']['Code'] == 'ThrottlingException':\n",
        "                if attempt < max_retries - 1:\n",
        "                    wait_time = (2 ** attempt) + random.uniform(0, 1)\n",
        "                    print(f\"Throttled! Waiting {wait_time:.1f} seconds...\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    raise\n",
        "            else:\n",
        "                raise\n",
        "    return None\n",
        "\n",
        "state = \"Georgia\"\n",
        "\n",
        "print(f\"Processing {state}...\")\n",
        "\n",
        "# Replace placeholder in template\n",
        "current_prompt = prompt_template.replace(\"{state_name}\", state)\n",
        "\n",
        "try:\n",
        "    # Call with retry logic\n",
        "    responses = call_bedrock_with_retry(bedrock, current_prompt)\n",
        "    \n",
        "    # Parse and store responses\n",
        "    for response in responses:\n",
        "        print(json.loads(response))\n",
        "    \n",
        "    print(f\"✓ Successfully processed {state}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ Failed to process {state}: {str(e)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Best Practices Summary\n",
        "\n",
        "### ✅ DO's:\n",
        "\n",
        "1. **Always use `bedrock-runtime` client** for model invocation\n",
        "2. **Convert body to JSON string** with `json.dumps()` before calling `invoke_model()`\n",
        "3. **Implement retry logic** with exponential backoff for ThrottlingException\n",
        "4. **Add rate limiting** between requests (e.g., `time.sleep(1)`)\n",
        "5. **Use prompt templates** for batch processing\n",
        "6. **Parse responses carefully** - remember to read the body and extract from content array\n",
        "7. **Handle errors gracefully** - continue processing other items if one fails\n",
        "8. **Add random jitter** to backoff times to prevent thundering herd\n",
        "9. **Only retry ThrottlingException** - re-raise other errors immediately\n",
        "10. **Monitor your usage** and adjust rate limiting based on your limits\n",
        "\n",
        "### ❌ DON'Ts:\n",
        "\n",
        "1. **Don't use `bedrock` client** for model invocation (use `bedrock-runtime`)\n",
        "2. **Don't pass Python dict** directly to `invoke_model()` - must be JSON string\n",
        "3. **Don't make rapid-fire requests** without delays\n",
        "4. **Don't retry all errors** - only retry ThrottlingException\n",
        "5. **Don't ignore throttling errors** - implement proper retry logic\n",
        "6. **Don't forget to read response body** - use `.read()` before parsing\n",
        "7. **Don't process large batches** without rate limiting\n",
        "8. **Don't hardcode wait times** without jitter - causes synchronized retries\n",
        "9. **Don't skip error handling** - one failure shouldn't stop the entire batch\n",
        "10. **Don't assume responses are always valid JSON** - wrap parsing in try-except\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Common Errors and Solutions\n",
        "\n",
        "### Error: `ClientError: Invalid request body`\n",
        "**Cause**: Passing Python dict instead of JSON string\n",
        "**Solution**: Use `json.dumps(body)` before calling `invoke_model()`\n",
        "\n",
        "### Error: `ThrottlingException: Too many requests`\n",
        "**Cause**: Making too many requests too quickly\n",
        "**Solution**: \n",
        "- Add `time.sleep(1)` between requests\n",
        "- Implement exponential backoff retry logic\n",
        "- Reduce batch size or increase delays\n",
        "\n",
        "### Error: `AttributeError: 'StreamingBody' object has no attribute 'read'`\n",
        "**Cause**: Trying to parse response body without reading it first\n",
        "**Solution**: Use `response[\"body\"].read()` before `json.loads()`\n",
        "\n",
        "### Error: `KeyError: 'content'`\n",
        "**Cause**: Response structure doesn't match expected format\n",
        "**Solution**: Check response structure, may need to handle different response types\n",
        "\n",
        "### Error: `JSONDecodeError` when parsing response\n",
        "**Cause**: Model returned non-JSON text despite requesting JSON\n",
        "**Solution**: \n",
        "- Wrap JSON parsing in try-except\n",
        "- Improve prompt to be more explicit about JSON format\n",
        "- Validate response before parsing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Key Takeaways\n",
        "\n",
        "### Critical Concepts:\n",
        "\n",
        "1. **Bedrock Runtime Client**: Use `boto3.client('bedrock-runtime')` for model invocation\n",
        "\n",
        "2. **Request Body Format**: Must include `anthropic_version: \"bedrock-2023-05-31\"` and properly formatted messages\n",
        "\n",
        "3. **JSON String Requirement**: Body must be `json.dumps(body)`, not a Python dictionary\n",
        "\n",
        "4. **ThrottlingException**: Expected when making many requests - implement retry logic, don't panic\n",
        "\n",
        "5. **Exponential Backoff**: `(2 ** attempt) + random.uniform(0, 1)` provides effective retry strategy\n",
        "\n",
        "6. **Rate Limiting**: Always add delays between requests in loops (`time.sleep(1)`)\n",
        "\n",
        "7. **Response Parsing**: `result[\"content\"][0][\"text\"]` extracts text from Bedrock responses\n",
        "\n",
        "8. **Error Handling**: Only retry ThrottlingException, re-raise other errors immediately\n",
        "\n",
        "9. **Prompt Templates**: Use templates with placeholders for batch processing\n",
        "\n",
        "10. **Production Readiness**: Always implement retry logic and rate limiting in production code\n",
        "\n",
        "### The Real-World Lesson:\n",
        "\n",
        "> \"The throttling we encountered when processing 50 states is a perfect example of why you need to plan for rate limits and implement proper error handling. The script successfully processed 47 states before hitting the limit, demonstrating that even legitimate, well-intentioned usage can trigger throttling protection.\"\n",
        "\n",
        "### Next Steps:\n",
        "- Practice with your own batch processing tasks\n",
        "- Experiment with different rate limiting strategies\n",
        "- Build robust error handling into your API integrations\n",
        "- Apply these patterns to other AWS services that may throttle\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
